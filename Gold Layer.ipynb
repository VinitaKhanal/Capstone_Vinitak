{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1236fe7c-d97b-4f89-8c7e-2f3cda6370d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Gold Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a91f07-a7c9-48dd-a4de-e26ea24b5704",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "*sourcing config file* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9f67ff3-302f-431d-b90a-282c496d5226",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /capstone/config_script/setup_schema_and_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad23ad0c-18c8-478a-9861-edbfecf11f77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "###Reading source data from silver layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86eb001c-c863-4f89-855a-ca16493ae623",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer input path: dbfs:/FileStore/capstone/cust_tbl/\nBranch input path: dbfs:/FileStore/capstone/branches_tbl/\nTransaction input path: dbfs:/FileStore/capstone/txn_tbl/\nCheckpoint location: dbfs:/FileStore/capstone/txn_tbl/checkpoint_location/\nCheckpoint location silver txn: dbfs:/FileStore/capstone/checkpoint/silver/txn/\nCheckpoint location cust txn: dbfs:/FileStore/capstone/checkpoint/silver/cust_txn/\nCheckpoint location cust seg : dbfs:/FileStore/capstone/checkpoint/silver/customer_segments/\nCheckpoint location fraud flag : dbfs:/FileStore/capstone/checkpoint/silver/fraud_flag/\nCheckpoint location gold fraud flag : dbfs:/FileStore/capstone/checkpoint/gold/farud_flag/\nCheckpoint location gold cust seg : dbfs:/FileStore/capstone/checkpoint/gold/customer_segments/\nColumns for null check: ['transaction_id', 'customer_id']\nExpected customer ID length: 5\nOrdered fraud flag columns: ['flag_id', 'transaction_id', 'flag_type', 'timestamp', 'confidence_score']\nOrdered customer segments columns: ['customer_id', 'segment_name', 'segment_description', 'last_updated_date']\nbronze_tables['txn'] is: bronze.txn_tbl\nbronze_tables['cust'] is: bronze.cust_tbl\nbronze_tables['branch'] is: bronze.branches_tbl\n"
     ]
    }
   ],
   "source": [
    "fraud_flag_df = spark.readStream.format(\"delta\").table(silver_tables['fraud_flag'])\n",
    "customer_segments_df = spark.readStream.format(\"delta\").table(silver_tables['customer_segments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d2573a-ad37-4741-92e2-657627c578fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Aggregating customer in different Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81f0caf9-5ab1-4099-be03-b2feeb2d7950",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f384e4234c0>"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp, count, first, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import uuid\n",
    "\n",
    "# Add a timestamp column\n",
    "customer_segments_df_with_timestamp = customer_segments_df.withColumn(\"event_timestamp\", current_timestamp())\n",
    "\n",
    "# Apply watermark on the new timestamp column\n",
    "df_with_watermark = customer_segments_df_with_timestamp.withWatermark(\"event_timestamp\", \"10 minutes\")\n",
    "\n",
    "# Perform aggregation\n",
    "customer_segments_result_df = df_with_watermark.groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"segment_count\"),\n",
    "        first(\"segment_name\").alias(\"segment_name\"),\n",
    "        first(\"segment_description\").alias(\"segment_description\"),\n",
    "        first(\"event_timestamp\").alias(\"last_updated_date\")  # Using the new timestamp column\n",
    "    )\n",
    "\n",
    "# Drop 'segment_count' column and add UUID\n",
    "def generate_uuid():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "uuid_udf = udf(generate_uuid, StringType())\n",
    "\n",
    "customer_segments_result_df = customer_segments_result_df.drop(\"segment_count\").withColumn('segment_id', uuid_udf())\n",
    "\n",
    "# Reorder the columns\n",
    "ordered_columns = ['segment_id', 'customer_id', 'segment_name', 'segment_description', 'last_updated_date']\n",
    "df_ordered = customer_segments_result_df.select(ordered_columns)\n",
    "\n",
    "# Write the DataFrame to a Delta table\n",
    "df_ordered.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_location_gold_customer_segments) \\\n",
    "    .table(gold_tables['gold_customer_segments'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfb1fca5-45ad-49df-8cae-f18426f89bdf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Fraud Flag table creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af0cad0-83db-4c96-87fc-c41c7743edd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f384f0c7af0>"
     ]
    }
   ],
   "source": [
    "fraud_flag_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_location_gold_fraud_flag) \\\n",
    "    .table(gold_tables['gold_fraud_flag'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff2bad56-de4b-4a7a-aa02-d4d330bc4d24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Checking intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d9ac647-100f-43d8-b442-50ccb2b011f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n|      flag_type|count(1)|\n+---------------+--------+\n|pattern_anomaly|       2|\n| unusual_amount|      18|\n|watchlist_match|     176|\n+---------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "result_flag_df = spark.sql(\"\"\"\n",
    "    SELECT flag_type, COUNT(*)\n",
    "    FROM gold.fraud_flag\n",
    "    GROUP BY flag_type\n",
    "\"\"\")\n",
    "\n",
    "result_flag_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb80264-fdff-4eff-9389-559a3256de1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n|segment_name|count(1)|\n+------------+--------+\n|  High_Value|      17|\n| Credit_Risk|      27|\n|    New_User|       6|\n|       Loyal|      13|\n+------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "result_seg_df = spark.sql(\"\"\"\n",
    "    SELECT segment_name, COUNT(*)\n",
    "    FROM gold.customer_segments\n",
    "    GROUP BY segment_name\n",
    "\"\"\")\n",
    "\n",
    "result_seg_df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Gold Layer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
