{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "565a0967-c508-439e-89e5-494452e0e164",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Silver Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f4044e-27e2-4f0a-bfe6-202ccef61ba9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "*sourcing config file* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e540ef8c-0fdd-43ec-8c14-ad4e8f472f30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run /capstone/config_script/setup_schema_and_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5cdb7f3-6dfb-4625-a551-362e0e336216",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>158</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         158
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer input path: dbfs:/FileStore/capstone/cust_tbl/\nBranch input path: dbfs:/FileStore/capstone/branches_tbl/\nTransaction input path: dbfs:/FileStore/capstone/txn_tbl/\nCheckpoint location: dbfs:/FileStore/capstone/txn_tbl/checkpoint_location/\nColumns for null check: ['transaction_id', 'customer_id']\nExpected customer ID length: 5\nOrdered fraud flag columns: ['flag_id', 'transaction_id', 'flag_type', 'timestamp', 'confidence_score']\nOrdered customer segments columns: ['customer_id', 'segment_name', 'segment_description', 'last_updated_date']\nbronze_tables['txn'] is: bronze.txn_tbl\nbronze_tables['cust'] is: bronze.cust_tbl\nbronze_tables['branch'] is: bronze.branches_tbl\n"
     ]
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "select count(*) from bronze.txn_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b50d5cf-7591-4884-8113-3734a413452e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Function to be used in silver notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f71200ea-19d3-4d0d-8caf-c6f00405c629",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Function drop duplicates from each source dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00e5d58a-0862-4dd1-b6eb-9f5b4d204289",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(input_df):\n",
    "\n",
    "    deduplicated_df = input_df.dropDuplicates()\n",
    "    return deduplicated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6803e59e-dde5-4d2e-a4d9-33bac6c0030c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Function to remove all specified columns which can't be null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf2eec7-49a9-404e-8cd7-df4b3cb09daa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def remove_null_rows(input_df, columns_to_check):\n",
    "    from pyspark.sql.functions import col\n",
    "    \n",
    "    # Filter rows where any specified column has null value\n",
    "    condition = ~reduce(lambda x, y: x | y, (col(column).isNull() for column in columns_to_check))\n",
    "    non_null_df = input_df.filter(condition)\n",
    "    \n",
    "    return non_null_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9d19204-d71c-4133-a66f-d24bd7464335",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Function to check valid customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9035eaa5-1891-4911-994f-f5d7fb4d6e71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def check_customer_id_length(input_df, length):\n",
    "    result_df = input_df.withColumn(\n",
    "        \"customer_id_length_valid\",\n",
    "        F.when(F.length(F.col(\"customer_id\")) == length, \"valid\").otherwise(\"invalid\"),\n",
    "    )\n",
    "    result_df = result_df.filter(col(\"customer_id_length_valid\")==\"valid\")\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf750a9a-7cc6-4d75-b545-ca018bc7bc2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Function for finding new user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70a372c2-f4f2-4ec3-98eb-9b2673de622f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def filter_recent_customers(join_date_col):\n",
    "    # Calculate the date one month ago\n",
    "    one_month_ago = F.date_sub(F.current_date(), 30)\n",
    "    \n",
    "    # Return a boolean condition for recent customers\n",
    "    return F.col(join_date_col) >= one_month_ago\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e96bceb7-87f0-4d8b-9831-ad20fd387f2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Function for finding invalid user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9e67aaa-c164-4694-a7ce-26433237c387",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def filter_old_customers(join_date_col):\n",
    "    # Calculate the date 90 days ago\n",
    "    ninety_days_ago = F.date_sub(F.current_date(), 90)\n",
    "    \n",
    "    # Return a boolean condition for customers older than 90 days\n",
    "    return F.col(join_date_col) <= ninety_days_ago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f88d2e19-8cc9-4656-8f72-d68948ae6852",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Function for new geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b485a207-7ca6-476c-b2b2-a09ad8853412",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to check if the zip code exists in the predefined list\n",
    "def check_zip_code(address):\n",
    "    zip_code = address[-5:]  # Extract last 5 digits\n",
    "    return zip_code in new_zip_codes_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97567385-17d5-4f12-8e69-098953c25506",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Reading source data from bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa74c85d-b936-49f7-ae3a-507e5e0a3ba6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# txn_df = spark.read.table(bronze_tables['txn'])\n",
    "txn_df = spark.readStream.format(\"delta\").table(bronze_tables['txn'])\n",
    "cust_df = spark.read.table(bronze_tables['cust'])\n",
    "branch_df = spark.read.table(bronze_tables['branch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a428343-f011-41bd-9634-70f2661a6dac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Strating to cleanse data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83d35625-e85c-43e8-bd33-c349c92ce236",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1.Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21dbb959-54b2-4449-8f90-b8740b45e58a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "txn_deduplicated_df = remove_duplicates(txn_df)\n",
    "cust_deduplicated_df = remove_duplicates(cust_df)\n",
    "branch_deduplicated_df = remove_duplicates(branch_df)\n",
    "\n",
    "# # Count the number of rows in each DataFrame\n",
    "# txn_count = txn_deduplicated_df.count()\n",
    "# cust_count = cust_deduplicated_df.count()\n",
    "# branch_count = branch_deduplicated_df.count()\n",
    "\n",
    "# # Print the counts\n",
    "# print(f\"Transaction DataFrame Count: {txn_count}\")\n",
    "# print(f\"Customer DataFrame Count: {cust_count}\")\n",
    "# print(f\"Branch DataFrame Count: {branch_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5628b1a8-f4c6-42ac-bcf8-ae03dfd7774b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "2. Dropping data if null value are present in mandatory cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d13e60ba-74f3-4ae9-9c20-14e9a1c4220c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove columns with null values based on specified columns\n",
    "txn_non_null_df = remove_null_rows(txn_deduplicated_df, columns_for_null_check)\n",
    "# txn_non_null_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93b77dd-7317-4546-8d8d-86d61badcf33",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "3. Check on valid customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37337a75-c5cf-4e9d-b5f8-c2b01a82c172",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Apply the function to check customer_id length\n",
    "verified_txn_df = check_customer_id_length(txn_non_null_df, expected_cust_id_length)\n",
    "verified_cust_df = check_customer_id_length(cust_deduplicated_df, expected_cust_id_length)\n",
    "\n",
    "# # Count the number of rows in each DataFrame\n",
    "# txn_count = verified_txn_df.count()\n",
    "# cust_count = verified_cust_df.count()\n",
    "\n",
    "# # Print the counts\n",
    "# print(f\"Transaction DataFrame Count: {txn_count}\")\n",
    "# print(f\"Customer DataFrame Count: {cust_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27199d6e-6e43-4f7b-952b-c5505a4d63a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Joining transaction,customer and branch data to produce customer segment and get fraud data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca6592c4-da24-4d7f-9d58-ea20dedffda8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joined_df = verified_txn_df.join(verified_cust_df, on=['customer_id', 'customer_id'], how='inner').join(branch_df, on=['branch_id', 'branch_id'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7874f00b-f8fb-4442-b6e0-cdd2a23ffb16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Zip code check for new geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc7ff56-f666-4619-a831-ead5ef1f39b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flag_source_df = joined_df.select(\"transaction_id\",\"amount\",\"channel\",\"timestamp\",\"currency\",\"address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a7f8c7d-c2d4-4e45-a28f-5fcbdd48e03f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "\n",
    "# Regular expression pattern to extract 5-digit ZIP code at the end\n",
    "zip_code_pattern = r'(\\d{5})$'\n",
    "\n",
    "# Add a new column 'zip_code' by applying the regular expression pattern\n",
    "df_with_zip = flag_source_df.withColumn(\"zip_code\", regexp_extract(\"address\", zip_code_pattern, 1))\n",
    "\n",
    "# zip_codes_list = df_with_zip.select(\"zip_code\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "\n",
    "# # Original list of zip codes\n",
    "# zip_codes_list = [f'{i:05d}' for i in range(1000)]  # Example list of zip codes \n",
    "# new_zip_codes_list = zip_codes_list[:955]\n",
    "\n",
    "new_zip_codes_list = ['00000']\n",
    "# Register the UDF with Spark\n",
    "check_zip_code_udf = F.udf(check_zip_code, BooleanType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0189130-e34f-464d-8c4d-dff0db7280bb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Fraud Flag table creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07f51f1-43b5-46d9-9f91-98d916143d80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[283]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f2cfee60d00>"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import uuid\n",
    "\n",
    "flag_df = joined_df.select(\n",
    "    F.col('transaction_id'),\n",
    "    F.when(F.col('amount') > 90000, \"unusual_amount\")\n",
    "     .when(F.col('currency').isin(['CRYPTO']), \"watchlist_match\")\n",
    "     .when(check_zip_code_udf(F.col('address')), \"new_geolocation\")\n",
    "     .when(F.col('amount') < 0, \"pattern_anomaly\")\n",
    "     .otherwise(\"normal\")  \n",
    "     .alias('flag_type'),\n",
    "    F.current_timestamp().alias('timestamp')\n",
    ")\n",
    "\n",
    "flag_type_df = flag_df.withColumn(\n",
    "    'confidence_score',\n",
    "    F.when(F.col('flag_type') == 'unusual_amount', 0.75)\n",
    "     .when(F.col('flag_type') == 'watchlist_match', 0.95)\n",
    "     .when(F.col('flag_type') == 'new_geolocation', 0.80)\n",
    "     .when(F.col('flag_type') == 'pattern_anomaly', 0.70)          \n",
    "     .otherwise(0.5)  # Default confidence score for other cases\n",
    ")\n",
    "\n",
    "flagged_df = flag_type_df.filter(F.col(\"flag_type\") != \"normal\")\n",
    "\n",
    "def generate_uuid():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "uuid_udf = udf(generate_uuid, StringType())\n",
    "\n",
    "# Add UUID column to DataFrame\n",
    "flag_id_df = flagged_df.withColumn(\"flag_id\", uuid_udf())\n",
    "\n",
    "# flag_id_df = flagged_df.withColumn('flag_id',F.concat(F.lit(\"F000\"), F.monotonically_increasing_id()))\n",
    "\n",
    "\n",
    "# Reorder the columns\n",
    "df_ordered_flag = flag_id_df.select(ordered_fraud_flag_columns)\n",
    "\n",
    "\n",
    "\n",
    "# # Write the DataFrame to a Delta table\n",
    "# df_ordered.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_tables['fraud_flag'])\n",
    "\n",
    "df_ordered_flag.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"dbfs:/FileStore/capstone/txn_tbl/checkpoint/silver/fraud_flag/\") \\\n",
    "    .table(silver_tables['fraud_flag'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae89549d-4aea-4b09-97e6-579b29b87775",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Customer segment table Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9050ff5-1711-43ed-afe5-a45722515c60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[284]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f2cfee5b5e0>"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col,when,floor,to_date,lit,round,datediff\n",
    "\n",
    " \n",
    "joined_yr_df = joined_df.withColumn(\"years_diff\", round(datediff(col(\"timestamp\").cast(\"timestamp\"), to_date(col(\"join_date\"), \"yyyy-MM-dd\")) / lit(365.25)))\n",
    "\n",
    "cust_seg_df = joined_yr_df.select(\n",
    "    F.col('customer_id'),\n",
    "    F.col('join_date'),\n",
    "    F.when(filter_recent_customers('join_date'), \"New_User\")\n",
    "     .when(filter_old_customers('last_update'), \"Inactive\")\n",
    "     .when(F.col('amount') > 90000, \"High_Value\")\n",
    "     .when(F.col('years_diff') > 5, \"Loyal\")\n",
    "     .when(F.col('credit_score') < 510, \"Credit_Risk\")\n",
    "     .otherwise(\"normal\")  \n",
    "     .alias('segment_name'),\n",
    "    F.current_timestamp().alias('last_updated_date')\n",
    ")\n",
    "\n",
    "cust_seg_id_df = cust_seg_df.filter(F.col(\"segment_name\") != \"normal\")\n",
    "\n",
    "\n",
    "cust_seg_final_df = (\n",
    "    cust_seg_id_df\n",
    "    .withColumn(\n",
    "        'segment_description',\n",
    "        when(col(\"segment_name\") == \"High_Value\", \"Customers with high transaction volume\")\n",
    "        .when(col(\"segment_name\") == \"New_User\", \"Customers who joined in last 30 days\")\n",
    "        .when(col(\"segment_name\") == \"Inactive\", \"No transactions in last 90 days\")\n",
    "        .when(col(\"segment_name\") == \"Credit_Risk\", \"Customers with low credit scores\")\n",
    "        .when(col(\"segment_name\") == \"Loyal\", \"Consistent activity for over 5 years\")\n",
    "        .otherwise(\"Unknown\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Reorder the columns\n",
    "df_ordered = cust_seg_final_df.select(ordered_customer_segments_columns)\n",
    "\n",
    "# Write the DataFrame to a Delta table\n",
    "# df_ordered.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_tables['customer_segments'])\n",
    "\n",
    "df_ordered.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"dbfs:/FileStore/capstone/txn_tbl/checkpoint/customer_segments/\") \\\n",
    "    .table(silver_tables['customer_segments'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "209ad288-717b-4637-b65b-2436199ae4c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Writing data in Silver Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17b48e57-f064-4538-905f-061783fb2f71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[285]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f2cfee60130>"
     ]
    }
   ],
   "source": [
    "cust_txn_df = joined_df.select(ordered_cut_txn_columns)\n",
    "\n",
    "# cust_txn_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_tables['cust_txn'])\n",
    "\n",
    "cust_txn_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"dbfs:/FileStore/capstone/txn_tbl/checkpoint/silver/cust_txn/\") \\\n",
    "    .table(silver_tables['cust_txn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57e045cf-2dd2-4fb8-b125-1570112161b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[286]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f2cfee53e20>"
     ]
    }
   ],
   "source": [
    "# verified_txn_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_tables['txn'])\n",
    "\n",
    "\n",
    "# Write the streaming DataFrame to the Delta table\n",
    "verified_txn_df.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"dbfs:/FileStore/capstone/txn_tbl/checkpoint/verified_txn_df/\") \\\n",
    "    .table(silver_tables['txn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c43f4e-ca73-4465-abb9-618927138ba4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "verified_cust_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_tables['cust'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d6744ff-6310-43cb-821c-c2fb8d597bd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "branch_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_tables['branch'])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1596567336340049,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Silver layer",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
