{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbf86891-0759-44e3-9983-b8070771c8f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##Source Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34768e9f-6a1e-4766-82d1-6aa4425cbd0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Installing libraries for data genration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "868929ac-481f-4684-a2dd-9116c0250939",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nRequirement already satisfied: dbldatagen in /local_disk0/.ephemeral_nfs/envs/pythonEnv-23eb97dc-9bf9-4c49-8e99-75f75f7b98fa/lib/python3.9/site-packages (0.4.0.post1)\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nRequirement already satisfied: faker in /local_disk0/.ephemeral_nfs/envs/pythonEnv-23eb97dc-9bf9-4c49-8e99-75f75f7b98fa/lib/python3.9/site-packages (26.0.0)\nRequirement already satisfied: python-dateutil>=2.4 in /databricks/python3/lib/python3.9/site-packages (from faker) (2.8.2)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install dbldatagen\n",
    "%pip install faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22f7339-0152-4176-ad78-8bdea1045790",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "sourcing config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "681ee37f-6a2e-4148-a1d0-512b94bb98ca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run /capstone/config_script/setup_schema_and_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be504e2e-b0e5-4d23-9246-dbf07728d752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: False"
     ]
    }
   ],
   "source": [
    "dbutils.fs.rm(\"dbfs:/FileStore/capstone/txn_tbl/checkpoint_location/\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5af2e20b-8e15-434f-9a74-13ddeafbec89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: False"
     ]
    }
   ],
   "source": [
    "dbutils.fs.rm(\"dbfs:/FileStore/capstone/txn_tbl/\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03f5e666-5a38-4182-8663-5207c074b827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "drop table if exists bronze.txn_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f82872c9-7d40-4cf0-97ab-ebca3c3c9313",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: False"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer input path: dbfs:/FileStore/capstone/cust_tbl/\nBranch input path: dbfs:/FileStore/capstone/branches_tbl/\nTransaction input path: dbfs:/FileStore/capstone/txn_tbl/\nCheckpoint location: dbfs:/FileStore/capstone/txn_tbl/checkpoint_location/\nColumns for null check: ['transaction_id', 'customer_id']\nExpected customer ID length: 5\nOrdered fraud flag columns: ['flag_id', 'transaction_id', 'flag_type', 'timestamp', 'confidence_score']\nOrdered customer segments columns: ['customer_id', 'segment_name', 'segment_description', 'last_updated_date']\nbronze_tables['txn'] is: bronze.txn_tbl\nbronze_tables['cust'] is: bronze.cust_tbl\nbronze_tables['branch'] is: bronze.branches_tbl\n"
     ]
    }
   ],
   "source": [
    "#create schema before running and also delete this location before dem\n",
    "\n",
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/bronze.db/txn_tbl\" ,recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "414cbaa2-f01b-402c-a2c9-2908ee7d459c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Generating data using faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ddca400-cf57-46eb-a749-fcbf906ab91e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.datasets import make_regression\n",
    "import dbldatagen as dg\n",
    "from pyspark.sql import SparkSession\n",
    "from time import sleep\n",
    " \n",
    "class DataGenerator:\n",
    "    def __init__(self, num_customers=1000, num_branches=10, num_transactions=5000):\n",
    "        self.fake = faker.Faker()\n",
    "        self.num_customers = num_customers\n",
    "        self.num_branches = num_branches\n",
    "        self.num_transactions = num_transactions\n",
    "        self.spark = SparkSession.builder.appName(\"DataGeneration\").getOrCreate()\n",
    " \n",
    "    def format_phone_number(self):\n",
    "        phone_number = self.fake.phone_number()\n",
    "        digits = ''.join(filter(str.isdigit, phone_number))\n",
    "        return f\"({digits[:3]}) {digits[3:6]}-{digits[6:10]}\"\n",
    " \n",
    "    def random_date(self, start_date, end_date):\n",
    "        start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "        end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "        return start_dt + timedelta(days=random.randint(0, (end_dt - start_dt).days))\n",
    " \n",
    "    def generate_customers(self):\n",
    "        names = [self.fake.name() for _ in range(self.num_customers)]\n",
    "        first_names = [name.split()[0] for name in names]\n",
    "        last_names = [name.split()[-1] for name in names]\n",
    "        addresses = [self.fake.address().replace('\\n', ', ') for _ in range(self.num_customers)]\n",
    "        email_providers = [\"gmail.com\", \"yahoo.com\", \"outlook.com\", \"hotmail.com\", \"aol.com\"]\n",
    "        emails = [f\"{first.lower()}.{last.lower()}@{random.choice(email_providers)}\" for first, last in zip(first_names, last_names)]\n",
    "        phone_numbers = [self.format_phone_number() for _ in range(self.num_customers)]\n",
    "        join_dates = sorted([self.random_date(\"2019-01-01\", \"2024-07-19\").strftime(\"%Y-%m-%d\") for _ in range(self.num_customers)])\n",
    "        last_updates = [self.random_date(\"2024-07-20\", \"2024-07-31\").strftime(\"%Y-%m-%d %H:%M:%S\") for _ in range(self.num_customers)]\n",
    "        credit_scores = [random.randint(500, 850) for _ in range(self.num_customers)]\n",
    "        customer_ids = [f\"C{index+1000:04d}\" for index in range(self.num_customers)]\n",
    " \n",
    "        customers_df = pd.DataFrame({\n",
    "            \"customer_id\": customer_ids,\n",
    "            \"name\": names,\n",
    "            \"email\": emails,\n",
    "            \"phone\": phone_numbers,\n",
    "            \"address\": addresses,\n",
    "            \"credit_score\": credit_scores,\n",
    "            \"join_date\": join_dates,\n",
    "            \"last_update\": last_updates\n",
    "        })\n",
    " \n",
    "        return self.spark.createDataFrame(customers_df)\n",
    " \n",
    "    def generate_branches(self):\n",
    "        branch_ids = [f\"B{index:04d}\" for index in range(self.num_branches)]\n",
    "        branch_names = random.choices([\"Downtown Branch\", \"Central Branch\", \"North Branch\", \"East Branch\", \"West Branch\"], k=self.num_branches)\n",
    "        cities = [self.fake.city() for _ in range(100)]\n",
    "        branch_locations = random.choices(cities, k=self.num_branches)\n",
    "        branch_timezones = random.choices([\"EST\", \"GMT\", \"PST\", \"AEST\"], k=self.num_branches)\n",
    " \n",
    "        branches_df = pd.DataFrame({\n",
    "            \"branch_id\": branch_ids,\n",
    "            \"name\": branch_names,\n",
    "            \"location\": branch_locations,\n",
    "            \"timezone\": branch_timezones\n",
    "        })\n",
    " \n",
    "        return self.spark.createDataFrame(branches_df)\n",
    " \n",
    "    def generate_transactions(self, customer_ids, branch_ids):\n",
    "        transaction_ids = [f\"T{index:04d}\" for index in range(5000, 10000)]\n",
    "        X, y = make_regression(n_samples=self.num_transactions, n_features=1, noise=10)\n",
    "        amounts = np.clip(np.abs(y), 1, 10000)\n",
    " \n",
    "        num_outliers = int(0.05 * len(amounts))\n",
    "        outliers_indices = np.random.choice(len(amounts), num_outliers, replace=False)\n",
    "        amounts[outliers_indices] = np.random.uniform(10000, 100000, size=num_outliers)\n",
    "        amounts = np.round(amounts, 2)\n",
    " \n",
    "        status_values = [\"completed\"] * (int(0.9 * self.num_transactions)) + [\"pending\"] * int(0.1 * self.num_transactions)\n",
    "        random.shuffle(status_values)\n",
    " \n",
    "        random_intervals = np.random.randint(0, 720, size=self.num_transactions)\n",
    "        cumulative_intervals = np.cumsum(random_intervals)\n",
    "        start_date = pd.Timestamp(\"2023-01-01\")\n",
    "        random_timestamps = [start_date + pd.Timedelta(minutes=int(m)) for m in cumulative_intervals]\n",
    " \n",
    "        weights = [0.95, 0.05]\n",
    "\n",
    "        transactions_df = pd.DataFrame({\n",
    "            \"transaction_id\": transaction_ids,\n",
    "            \"customer_id\": random.choices(customer_ids, k=self.num_transactions),\n",
    "            \"branch_id\": random.choices(branch_ids, k=self.num_transactions),\n",
    "            \"channel\": random.choices([\"ATM\", \"web\", \"mobile\", \"branch\"], k=self.num_transactions),\n",
    "            \"transaction_type\": random.choices([\"withdrawal\", \"deposit\", \"transfer\", \"payment\"], k=self.num_transactions),\n",
    "            \"amount\": amounts,\n",
    "            \"currency\": random.choices([\"USD\",\"CRYPTO\"], weights=weights, k=self.num_transactions),\n",
    "            \"timestamp\": random_timestamps,\n",
    "            \"status\": status_values\n",
    "        })\n",
    " \n",
    "        return self.spark.createDataFrame(transactions_df)\n",
    " \n",
    "# Usage\n",
    "data_gen = DataGenerator()\n",
    " \n",
    "customers_spark_df = data_gen.generate_customers()\n",
    "branches_spark_df = data_gen.generate_branches()\n",
    "transactions_spark_df = data_gen.generate_transactions(customers_spark_df.select(\"customer_id\").rdd.flatMap(lambda x: x).collect(),\n",
    "                                                       branches_spark_df.select(\"branch_id\").rdd.flatMap(lambda x: x).collect())\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b1bc77-e974-41d8-8a6e-7469babb7d27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Writing source data produced in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b1ad643-845e-470a-bdf3-9a9e560d3e45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_spark_df.write.format(\"delta\").mode('overwrite').save(input_path_cust)\n",
    "branches_spark_df.write.format(\"delta\").mode('overwrite').save(input_path_branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44ba2cdb-c372-4daf-a310-894942529d0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#####Generating dummy data with anamolies as faker is generating data with no anamolies so all cleansing functions can be verified "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f438853-35e9-4949-a0d5-bedb503b182d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully written to Delta table 'bronze.txn_tbl'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Optimized Spark Code\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema with correct data types, using StringType for timestamp initially\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"branch_id\", StringType(), True),\n",
    "    StructField(\"channel\", StringType(), True),\n",
    "    StructField(\"transaction_type\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"currency\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),  # Use StringType initially\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data with timestamps as strings\n",
    "data = [\n",
    "    ('T10001', 'C2000', 'B0008', 'branch', 'payment', 1000000.0, 'GBP', '2023-01-01 04:16:00', 'completed'),\n",
    "    ('T10001', 'C2000', 'B0008', 'branch', 'payment', 1000000.0, 'GBP', '2023-01-01 04:16:00', 'completed'),\n",
    "    (None, 'C2000', 'B0008', 'branch', 'payment', 1000000.0, 'GBP', '2023-01-01 04:16:00', 'completed'),\n",
    "    ('T10004', 'C2001', 'B0008', 'branch', 'payment', 17000.0, 'GBP', '2023-01-01 04:16:00', 'completed'),\n",
    "    ('T10005', 'C1999', 'B0008', 'branch', 'payment', -1000000.0, 'GBP', '2023-01-01 04:16:00', 'completed'),\n",
    "    ('T10006', 'C2001', 'B0008', 'branch', 'payment', 120000.0, 'GBP', '2023-01-01 04:16:00', 'completed'),\n",
    "    ('T10007', 'C201', 'B0008', 'branch', 'payment', 10000.0, 'GBP', '2023-01-01 04:16:00', 'completed'),\n",
    "    ('T10008', 'C1999', 'B0008', 'branch', 'payment', -10.0, 'GBP', '2023-01-01 04:16:00', 'completed')\n",
    "]\n",
    "\n",
    "# Create DataFrame with the defined schema\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Convert timestamp column to TimestampType\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(df[\"timestamp\"], \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "\n",
    "try:\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(bronze_tables['txn'])\n",
    "    print(f\"Data successfully written to Delta table '{bronze_tables['txn']}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while writing to Delta table: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2d6e2bb-7910-41a5-8e65-d1fb3f34372a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "####Creating Streaming files for Transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61e656c-9103-470b-bd7e-66fc3120d40d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\nWrote 50 records. Waiting 10 seconds...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "transactions_pandas_df = transactions_spark_df.orderBy(\"transaction_id\").toPandas()\n",
    "\n",
    "# Split into 5 chunks \n",
    "chunks = np.array_split(transactions_pandas_df, 100)\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Convert Pandas DataFrame back to Spark DataFrame\n",
    "    chunk_spark_df = spark.createDataFrame(chunk)\n",
    "    \n",
    "    # Write the chunk to Delta table\n",
    "    chunk_spark_df.write.format(\"delta\").mode(\"append\").save(input_path_txn)\n",
    "    print(f\"Wrote {len(chunk)} records. Waiting 10 seconds...\")\n",
    "    \n",
    "    # Sleep for 10 seconds\n",
    "    time.sleep(10)\n",
    "\n",
    "print(\"Completed writing txn data\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3127405337210312,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "SourceData Generation Script",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
